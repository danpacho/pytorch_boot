{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH1. Tensor\n",
    "\n",
    "1. Tensor란, vector, matrix 등 다차원 행렬에 대한 일반화된 개념입니다.\n",
    "2. 다차원 행렬로 표현되며, `NxMxOxPx...`의 차원 형태로 표현됩니다.\n",
    "3. Data의 기본적인 단위이며, 차원의 결합을 중요하게 생각해야 합니다.\n",
    "4. 모든 수학적 연산이 다음을 통해 이루어진다고 이해할 수 있습니다.\n",
    "\n",
    "## 1. Tensor의 표현\n",
    "\n",
    "아래 사진과 같이 Tensor는 어떤 데이터의 조직화된 형태를 나타냅니다. 3차원까지는 직육면체와 같이 적층된 형태로 이해할 수 있지만, 4차원 이상의 tensor 컴포넌트는 시각화가 어렵습니다. \n",
    "\n",
    "이에 3차원 tensor의 모음을 4차원 tensor로 표현할 수 있다와 같이, 다차원 tensor는 하위 tensor의 집합으로 이해할 수 있을 것입니다.\n",
    "\n",
    "![텐서](../../assets/1.png)\n",
    "\n",
    "```bash\n",
    "Scalar -> Vector(1D Tensor) -> Matrix(2D Tensor) -> ... N-dim Tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data와 Tensor\n",
    "\n",
    "앞서 살펴본 바대로, Data는 수학적 표현으로 추상화 할 수 있습니다.\n",
    "\n",
    "**이때 tensor의 row, col, depth 등의 각 차원 개념이 어떻게 data의 특징을 조직화 하고 있는지 반드시 이해해야 합니다.**\n",
    "\n",
    "### Vector의 조합 = Matrix\n",
    "\n",
    "1개의 data 형태 = vector 인경우를 생각해봅시다.\n",
    "\n",
    "예컨대 \n",
    "1. Fluid flow에 대한 simulation data\n",
    "    다음과 같은 snapshot data로 표현될 수 있을 겁니다.\n",
    "    ![snapshot](../../assets/2.png)\n",
    "    \n",
    "    본 data는 image로 flattened vector로 표현될 수 있습니다.(비록 data의 공간적 정보를 상실하겠지만)\n",
    "    \n",
    "    위 같은 경우, 각 `colum` vector = snapshot of fluid flow\n",
    "    각 `row` = state of time 의 의미를 갖게 될 것입니다.\n",
    "\n",
    "    ```python\n",
    "    [\n",
    "        [snapshot_1_image_vector], <--- t = 0\n",
    "        [snapshot_2_image_vector], <--- t = 0.25\n",
    "        [snapshot_3_image_vector], <--- t = 0.5\n",
    "        ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "2. User의 preference에 대한 data\n",
    "    혹은 user의 preference에 대한 data가 있다고 가정해봅시다.\n",
    "    예컨대 Netflix의 모든 영화에 대해 어떤 유저가 평점을 남겼다고 생각해봅시다.\n",
    "    그렇다면 각 `column` vector는 모든 영화에 대한 평점을 의미하게 될 것이며, 다음과 같이 나타날 것입니다.\n",
    "\n",
    "    ```python\n",
    "    [5.5, 4.5, 3.5, 2.5, 1.5, 0.5]_T ...\n",
    "    ```\n",
    "\n",
    "    그리고 모든 Netflix 유저가 모인다면,\n",
    "    각 `row` = user를 대변하게 될 것입니다.\n",
    "    \n",
    "    ```python\n",
    "    [\n",
    "        [5.5, 4.5, 3.5, 2.5, 1.5, 0.5]_T, <-- 철수\n",
    "        [4.5, 3.5, 2.5, 1.5, 0.5, 5.5]_T, <-- 영희\n",
    "        [3.5, 2.5, 1.5, 0.5, 5.5, 4.5]_T, <-- 민수\n",
    "        ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "이처럼 data의 dimension이 의미하는 바를 깊이 이해할 수록 정확한 모델링이 가능해질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensor와 다차원 행렬\n",
    "\n",
    "수학적으로는 $$\\mathbb{R}^{N \\times M \\times O ...}$$의 형태로 표현되는 `NXMXOX...` Tensor는, 컴퓨터로 표현하고자 할 때 다차원 행렬을 이용합니다.\n",
    "\n",
    "예컨대 3차원 tensor는 3차원 행렬로 표현될 수 있습니다.\n",
    "\n",
    "```python\n",
    "[\n",
    "    [\n",
    "        [1, 2, 3], # vector = 1D tensor\n",
    "        [4, 5, 6], # vector = 1D tensor\n",
    "        [7, 8, 9], # vector = 1D tensor\n",
    "    ], # matrix = 2D tensor\n",
    "    [\n",
    "        [10, 11, 12],  # vector = 1D tensor\n",
    "        [13, 14, 15],  # vector = 1D tensor\n",
    "        [16, 17, 18],  # vector = 1D tensor\n",
    "    ], # matrix = 2D tensor\n",
    "    ...\n",
    "] # 3D tensor = matrix의 조합\n",
    "```\n",
    "\n",
    "앞서 살펴본 tensor의 직관적 이해와 동일하게, 3D tensor는 2D tensor, 즉 matrix의 집합으로 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 `numpy`, `torch`를 통한 Tensor 표현\n",
    "\n",
    "앞서 살펴본 tensor은 다차원 배열을 통해 정의할 수 있었습니다. 그럼에도 순수한 다차원 행렬은 Linear algebra의 연산을 지원하기에 다소 부족합니다.\n",
    "\n",
    "이에 이미 모든 수학적 연산이 정의된 `numpy`, `torch` 등의 라이브러리를 통해 tensor를 표현하고 쉽게 다룰 수 있습니다.\n",
    "\n",
    "### 4.1.1 `numpy`를 통한 Tensor 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1D tensor\n",
    "벡터 = np.array([1, 2, 3])\n",
    "\n",
    "# 2D tensor\n",
    "행렬 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 3D tensor\n",
    "tensor3D = np.array(\n",
    "    [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]\n",
    ")\n",
    "\n",
    "# 4D tensor\n",
    "tensor4D = np.array(\n",
    "    [\n",
    "        [\n",
    "            [[19, 20, 21], [22, 23, 24], [25, 26, 27]],  # 2D tensor\n",
    "            [[28, 29, 30], [31, 32, 33], [34, 35, 36]],  # 2D tensor\n",
    "        ],\n",
    "        [  # 3D tensor\n",
    "            [[19, 20, 21], [22, 23, 24], [25, 26, 27]],  # 2D tensor\n",
    "            [[28, 29, 30], [31, 32, 33], [34, 35, 36]],  # 2D tensor\n",
    "        ],  # 3D tensor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 `torch`를 통한 Tensor 표현\n",
    "\n",
    "pytorch는 numpy의 `array`와 유사한 동작을 제공하는 `torch.Tensor`를 제공합니다.\n",
    "\n",
    "`Tensor`은 특히 data의 type을 정의할 수 있으며, `Int`, `Float`, `Double` 등의 type을 지원합니다.\n",
    "\n",
    "```python\n",
    "(dtype=torch.int32, ...)\n",
    "# 명시적으로 data type을 정의하여 효율적인 operation을 가능케 합니다.\n",
    "```\n",
    "\n",
    "> torch의 tensor는 `numpy`의 `array`와 물리적 공간을 공유하기에, 한 배열의 reference를 직접 수정하게 된다면, torch와 numpy는 상호적으로 변경될 것입니다.\n",
    ">\n",
    "> 즉 pointer를 공유한다는 의미로 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[19, 20, 21],\n",
      "          [22, 23, 24],\n",
      "          [25, 26, 27]],\n",
      "\n",
      "         [[28, 29, 30],\n",
      "          [31, 32, 33],\n",
      "          [34, 35, 36]]],\n",
      "\n",
      "\n",
      "        [[[19, 20, 21],\n",
      "          [22, 23, 24],\n",
      "          [25, 26, 27]],\n",
      "\n",
      "         [[28, 29, 30],\n",
      "          [31, 32, 33],\n",
      "          [34, 35, 36]]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1D tensor\n",
    "벡터 = torch.IntTensor([1, 2, 3])\n",
    "\n",
    "# 2D tensor\n",
    "행렬 = torch.IntTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 3D tensor\n",
    "tensor3D = torch.IntTensor(\n",
    "    [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]\n",
    ")\n",
    "\n",
    "# 4D tensor\n",
    "tensor4D = torch.IntTensor(\n",
    "    [\n",
    "        [\n",
    "            [[19, 20, 21], [22, 23, 24], [25, 26, 27]],  # 2D tensor\n",
    "            [[28, 29, 30], [31, 32, 33], [34, 35, 36]],  # 2D tensor\n",
    "        ],\n",
    "        [  # 3D tensor\n",
    "            [[19, 20, 21], [22, 23, 24], [25, 26, 27]],  # 2D tensor\n",
    "            [[28, 29, 30], [31, 32, 33], [34, 35, 36]],  # 2D tensor\n",
    "        ],  # 3D tensor\n",
    "    ]\n",
    ")\n",
    "print(tensor4D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 tensor의 수학적 속성 살펴보기\n",
    "\n",
    "**tensor는 다음과 같은 수학적 속성을 가집니다.**\n",
    "\n",
    "1. `shape` : tensor의 차원을 나타내는 속성\n",
    "2. pytorch = `rank`, numpy = `ndim` : tensor의 차원의 수를 나타내는 속성\n",
    "3. pytorch = `numel`, numpy =  `size` : tensor의 element의 개수를 나타내는 속성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2, 3)\n",
      "Shape: torch.Size([2, 3])\n",
      "Dimension: 2\n",
      "Dimension: 2\n",
      "Size: 6\n",
      "Size: 6\n"
     ]
    }
   ],
   "source": [
    "# numpy, pytorch mathematical properties\n",
    "numpyMatrix = np.array([[1, 2, 3], [1, 2, 3]])\n",
    "torchMatrix = torch.IntTensor([[1, 2, 3], [1, 2, 3]])\n",
    "\n",
    "# shape\n",
    "print(\"Shape:\", numpyMatrix.shape)\n",
    "print(\"Shape:\", torchMatrix.shape)\n",
    "# (3,) = (1, 3) 과 같습니다.\n",
    "\n",
    "# rank (number of dimensions)\n",
    "print(\"Dimension:\", numpyMatrix.ndim)\n",
    "print(\"Dimension:\", torchMatrix.dim())\n",
    "\n",
    "# size\n",
    "print(\"Size:\", numpyMatrix.size)\n",
    "print(\"Size:\", torchMatrix.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 tensor 객체의 속성 살펴보기\n",
    "\n",
    "**tensor 객체에 대한 속성을 다음과 같습니다**\n",
    "\n",
    "1. `dtype` : tensor의 data type을 나타내는 속성\n",
    "2. `device` : tensor가 저장된 device를 나타내는 속성\n",
    "3. `requires_grad` : tensor의 gradient를 계산할 것인지를 나타내는 속성\n",
    "    gradient는 각 tensor의 특정 값에 대한 편미분 값으로, 각 tensor의 graph node의 변화를 관찰함으로써 계산됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: int64\n",
      "Data type: torch.int32\n",
      "Device: cpu\n",
      "'numpy.ndarray' object has no attribute 'requires_grad'\n",
      "Int tensor는 requires_grad를 지원하지 않습니다. 0으로 나눠질 수 있기 때문이죠.\n",
      "Requires grad of floatTensor: True\n"
     ]
    }
   ],
   "source": [
    "# data type\n",
    "print(\"Data type:\", numpyMatrix.dtype)\n",
    "print(\"Data type:\", torchMatrix.dtype)\n",
    "\n",
    "# device\n",
    "print(\"Device:\", torchMatrix.device)\n",
    "\n",
    "# requires_grad\n",
    "try:\n",
    "    numpyMatrix.requires_grad = True\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\n",
    "        \"Int tensor는 requires_grad를 지원하지 않습니다. 0으로 나눠질 수 있기 때문이죠.\"\n",
    "    )\n",
    "\n",
    "floatTensor = torch.FloatTensor([1.1, 2.1, 3.1])\n",
    "floatTensor.requires_grad = True\n",
    "\n",
    "# requires_grad\n",
    "print(\"Requires grad of floatTensor:\", floatTensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 tensor의 추출(slicing)\n",
    "\n",
    "tensor의 특정 부분을 적절히 추출하기 위해 Python의 slicing을 사용할 수 있습니다.\n",
    "이때 Slicing이란, \n",
    "```python\n",
    "sliced = array[start:stop:step]\n",
    "```\n",
    "와 같이 표현되며, `start`, `stop`, `step`은 각각 시작, 끝, 간격을 의미합니다.\n",
    "\n",
    "> 주의:\n",
    ">\n",
    "> index 기준 값으로 계산됩니다.\n",
    ">\n",
    ">`start`, `stop`, `step`은 모두 생략 가능하며, 생략시 각각 `0`, `len(array)`, `1`로 간주됩니다.\n",
    ">\n",
    "> `start`, `stop`, `step`은 모두 음수가 가능하며, 음수의 경우 뒤에서부터 시작한다는 의미입니다.\n",
    ">\n",
    "> **`start`는 포함되지만, `stop`은 포함되지 않습니다.**\n",
    "\n",
    "\n",
    "**`:` operator를 적절히 활용하여, 귀찮은 for문 없이도 tensor의 특정 부분을 쉽게 추출할 수 있습니다.**\n",
    "\n",
    "### 4.2.1 1D tensor의 slicing\n",
    "\n",
    "예컨대 1D tensor(vector)의 일부분을 추출하여 또다른 vector를 만들거나, 특정 Index의 scalar를 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n",
      "[[4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vec = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "sliced1 = vec[1:4]  # index 1, 2, 3 요소 추출\n",
    "print(sliced1)\n",
    "\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "secondRowVector = matrix[1:2]  # 2번째 row vector 추출\n",
    "print(secondRowVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 tensor의 일부를 slicing한 경우, 차원이 여전히 동일한 tensor가 반환됩니다.\n",
    "\n",
    "그렇다면, 어떻게 slicing을 통해 없는 차원을 제거할 수 있을까요?\n",
    "\n",
    "`numpy`의 `flatten` method를 통해 빈 차원을 제거할 수 있습니다.\n",
    "즉 `[[[1,2,3]]]` 과 같은 3D tensor를 `flatten`하면 `[1,2,3]`과 같은 1D tensor로 변환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "secondRowVector = matrix[1:2, :]  # 2번째 row vector 추출\n",
    "# [[4 5 6]] -> [4, 5, 6]\n",
    "extracted = secondRowVector.flatten()\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch`에서도 동일한 방법으로 `flatten`을 수행할 수 있습니다.\n",
    "\n",
    "확인할 수 있다시피, 거의 유사한 api를 제공하기에, 직관적으로 받아들일 수 있을것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.IntTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "secondRowVector = matrix[1:2]  # 2번째 row vector 추출\n",
    "extracted = secondRowVector.flatten()\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 tensor의 차원 변환\n",
    "\n",
    "tensor는 종종 저차원 혹은 고차원으로 변환되어야 하는 경우가 잦습니다.\n",
    "\n",
    "1. `numpy`의 경우\n",
    "    `reshape` method를 통해 tensor의 차원을 변환할 수 있습니다.\n",
    "    이때 `reshape` method는 원래 tensor의 element 개수와 변환하려는 tensor의 element 개수가 일치해야 합니다.\n",
    "\n",
    "2. `torch`의 경우\n",
    "    `view` method를 통해 tensor의 차원을 변환할 수 있습니다.\n",
    "    이때 `view` method는 `reshape`와 동일하게 원래 tensor의 element 개수와 변환하려는 tensor의 element 개수가 일치해야 합니다.\n",
    "\n",
    "이때 중요한 점은, 차원 변환 이후 전체 element의 요소는 동일해야 한다는 점입니다.\n",
    "\n",
    "어려운 것은 아니고, 예컨대 3x6x9로 이루어진 3D tensor를 2D tensor로 변환하고자 한다면, 요소의 전체 갯수인 162개가 변환 이후에도 동일해야 합니다.\n",
    "\n",
    "즉 `3x6x9` (= 162) = `2x81` = `18x9` = `6x27` = ... 등으로 원하는 matrix로 변환할 수 있습니다.\n",
    "\n",
    "이제 torch와 numpy를 통해 tensor를 다루는 방법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Numpy > reshape\n",
    "\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 3x3 matrix -> 1x9 vector\n",
    "vec = matrix.reshape(1, 9).flatten()\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch`에서는 `view` method를 통해 tensor의 차원을 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch > view\n",
    "\n",
    "matrix = torch.IntTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 3x3 matrix -> 1x9 vector\n",
    "\n",
    "vec = matrix.view(1, 9).flatten()\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 tensor의 기본적 연산\n",
    "\n",
    "Tensor의 연산은 일반적으로 알고 있는 matrix의 기본 연산 규칙과 동일합니다\n",
    "\n",
    "### tensor의 합, 차\n",
    "\n",
    "두 tensor의 차원이 동일해야 합니다.\n",
    "\n",
    "이때, `broadcasting`이라는 개념을 통해 tensor의 차원이 다른 경우에도 연산이 가능합니다.\n",
    "\n",
    "#### Broadcasting\n",
    "\n",
    "Broadcasting은 tensor의 차원이 다른 경우에도 연산이 가능하도록 하는 기능입니다.\n",
    "예컨대\n",
    "```python\n",
    "first = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "second = torch.tensor([1, 2, 3])\n",
    "third = torch.tensor([1])\n",
    "```\n",
    "\n",
    "원래 다음과 같은 2개의 tensor는 연산이 불가능합니다. 그러나 `broadcasting`을 통해 다음과 같이 연산이 가능해집니다.\n",
    "\n",
    "```python\n",
    "res1 = first + second\n",
    "# tensor([[2, 4, 6], [5, 7, 9]])\n",
    "res2 = first + third\n",
    "# tensor([[2, 3, 4], [5, 6, 7]])\n",
    "res3 = res1 + third\n",
    "# tensor([[3, 5, 7], [6, 8, 10]])\n",
    "```\n",
    "\n",
    "이는 `second` tensor가 `first` tensor의 각 row에 대해 더해지는 것을 의미합니다. 꽤 유용한 경우도 있으나, 원래의 수학적 연산과 의미과 다르고 **각 요소에 대해 적용하는 syntax sugar**이므로 사용해 주의해야 합니다.\n",
    "\n",
    "### tensor의 곱(내적)\n",
    "\n",
    "두 tensor의 차원이 `(N, M)`과 `(M, O)`일 때, `N x O`의 결과를 반환합니다.\n",
    "\n",
    "이때 다음과 같이 dot product를 진행할 수 있습니다.\n",
    "\n",
    "1. `torch.dot` : 두 tensor의 dot product를 반환합니다.\n",
    "2. `torch.matmul` : 두 tensor의 matrix multiplication을 반환합니다.\n",
    "3. `{Tensor}.dot({Tensor})` : 두 tensor의 dot product를 반환합니다.\n",
    "4. `@` : 두 tensor의 matrix multiplication을 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "first = np.array([1, 2, 3, 4, 5])\n",
    "second = np.array([6, 7, 8, 9, 10]).T\n",
    "dot = first.dot(second)\n",
    "dot = np.dot(first, second)\n",
    "dot = np.matmul(first, second)\n",
    "dot = first @ second\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(130, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "first = torch.IntTensor([1, 2, 3, 4, 5])\n",
    "second = torch.IntTensor([6, 7, 8, 9, 10]).t()\n",
    "dot = first.dot(second)\n",
    "dot = torch.dot(first, second)\n",
    "dot = torch.matmul(first, second)\n",
    "dot = first @ second\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor의 외적\n",
    "\n",
    "두 tensor의 차원이 `(N, M)`과 `(O, P)`일 때, `N x O x M x P`의 결과를 반환합니다.\n",
    "\n",
    "이때 다음과 같이 outer product를 진행할 수 있습니다.\n",
    "\n",
    "`torch.outer` : 두 tensor의 outer product를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  7  8  9 10]\n",
      " [12 14 16 18 20]\n",
      " [18 21 24 27 30]\n",
      " [24 28 32 36 40]\n",
      " [30 35 40 45 50]]\n"
     ]
    }
   ],
   "source": [
    "first = np.array([1, 2, 3, 4, 5])\n",
    "second = np.array([6, 7, 8, 9, 10])\n",
    "\n",
    "# outer product\n",
    "outer = np.outer(first, second)\n",
    "print(outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element-wise 연산\n",
    "\n",
    "Element-wise 연산이란, 두 tensor의 같은 위치에 있는 element끼리 연산을 진행하는 것을 의미합니다. 즉, 두 tensor의 차원이 동일해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 14 24 36 50]\n"
     ]
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "elementWise = first * second\n",
    "print(elementWise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 tensor의 concat, stack\n",
    "\n",
    "tensor의 차원을 변환하는 또다른 방법으로, 두 tensor를 2가지 방식으로 합치는 방법이 있습니다.\n",
    "\n",
    "### 4.5.1 tensor의 concat\n",
    "\n",
    "`concat`은 두 tensor를 합치는 방법으로, 두 tensor의 차원이 동일해야 합니다.\n",
    "\n",
    "**이는 concat이란 말의 의미처럼 두 tensor를 이어붙이는 것을 의미합니다.**\n",
    "\n",
    "1. numpy: `np.concatenate([tensor1, tensor2], axis={axis})`\n",
    "2. torch: `torch.cat([tensor1, tensor2], dim={dim})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Concat\n",
    "first = np.array([1, 2, 3])\n",
    "second = np.array([4, 5, 6])\n",
    "\n",
    "concat = np.concatenate((first, second), axis=0)\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 numpy의 예제의 경우 axis = 0으로 선언했습니다. 과연 이 의미는 무엇일까요?\n",
    "\n",
    "`axis`는 tensor의 차원을 나타내며, `axis = 0`은 첫번째 차원을 의미합니다.\n",
    "\n",
    "> tensor의 차원이란?\n",
    ">\n",
    "> tensor의 차원은 `NxMxOxP...`의 형태로 표현됩니다. 이때 axis는 `N`, `M`, `O`, `P` 등의 차원을 의미합니다.\n",
    "\n",
    "즉 `axis = 0`으로 선언하면, 두 tensor의 첫번째 차원을 이어붙이는 것을 의미합니다.\n",
    "\n",
    "만약 `axis = 1`로 선언하면, 두 tensor의 두번째 차원을 이어붙이는 것을 의미합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First axis concat: [[1 2 3]\n",
      " [4 5 6]]\n",
      "Second axis concat: [[1 2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Concat, axis=1\n",
    "first = np.array([[1, 2, 3]])\n",
    "second = np.array([[4, 5, 6]])\n",
    "\n",
    "# Concat, axis=0\n",
    "firstConcat = np.concatenate((first, second), axis=0)\n",
    "print(\"First axis concat:\", firstConcat)\n",
    "\n",
    "secondConcat = np.concatenate((first, second), axis=1)\n",
    "print(\"Second axis concat:\", secondConcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch의 경우도 동일한 방식으로 `dim`을 통해 tensor의 차원을 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "tensor([[1, 2, 3, 4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "first = torch.IntTensor([[1, 2, 3]])\n",
    "second = torch.IntTensor([[4, 5, 6]])\n",
    "\n",
    "firstConcat = torch.cat((first, second), dim=0)\n",
    "print(firstConcat)\n",
    "\n",
    "secondConcat = torch.cat((first, second), dim=1)\n",
    "print(secondConcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 tensor의 stack\n",
    "\n",
    "`stack`은 두 tensor를 쌓는 방법으로, 두 tensor의 차원이 동일해야 합니다.\n",
    "\n",
    "**이는 stack이란 말의 의미처럼 두 tensor를 층층이 쌓는 것을 의미합니다.**\n",
    "\n",
    "1. numpy: `np.stack([tensor1, tensor2], axis={axis})`\n",
    "2. torch: `torch.stack([tensor1, tensor2], dim={dim})`\n",
    "\n",
    "`stack`의 경우도 마찬가지로 axis, dim을 통해 쌓고자 하는 목표 tensor의 차원을 지정할 수 있습니다. \n",
    "\n",
    "즉 dim=0인 경우, 0차원 방향으로 쌓아라는 의미이며, 직관적으로는 y-stack과 같은 개념으로 이해할 수 있습니다. \n",
    "\n",
    "dim=1인 경우, 1차원 방향으로 쌓아라는 의미이며, x-stack과 같은 개념으로 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "vec1 = np.array([1, 2, 3])\n",
    "vec2 = np.array([4, 5, 6])\n",
    "\n",
    "stackedMatrix = np.stack([vec1, vec2], axis=0)\n",
    "print(stackedMatrix)\n",
    "\n",
    "# PyTorch\n",
    "vec1 = torch.IntTensor([1, 2, 3])\n",
    "vec2 = torch.IntTensor([4, 5, 6])\n",
    "\n",
    "stackedMatrix = torch.stack([vec1, vec2], dim=0)\n",
    "print(stackedMatrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
